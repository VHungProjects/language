{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learning\n",
    "The purpose of this notebook is to learn how language models are created.\n",
    "Unsure if this will ever be completed but this will document the some of the learning \n",
    "#### Resources Used\n",
    "https://huggingface.co/blog/how-to-train\n",
    "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Planned path\n",
    "- Find dataset\n",
    "- Train tokenizer\n",
    "- Train Language model\n",
    "- Check Training\n",
    "- Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
    "#### What is a Tokenizer?\n",
    "In short a process that breaks down text into smaller units. In general a token can refer to 3 different types:(\"Never give up\") words(never, give, up), characters(n,e,v,e,r,...) or sub-words(smart, er). This process breaks down sentences into seperate words or smaller.\n",
    "\n",
    "#### Why is this useful?\n",
    "Tokens have been used as the standard method of processing row text. Most(all?) NLP architectures process text at a token level. An RNN would process each token at a time.\n",
    "\n",
    "\"Creating vocabulary is the ultimate goal of tokenization\" I believe the meaning of this is for reasons similar to how humans learn a language, a vocabulary of words that we understand. We do this when we are young and dont always have a good grasp on this, however when learning a new language you must do this process again. For example when learning japanese recognizing hiragana and katakana are relatively simple and can probably be done within a few days, however learning Kanji and the meanings will require a vast amount of time.\n",
    "A short cut to creating vocabulary for a NLP model is using the top most frequently occuring words. \n",
    "\n",
    "Traditional NLP Vs. Advanced Deep Learning NLP\n",
    "Traditional uses vocabulary as features and each word is considered a unique feature.\n",
    "Deep Learning uses vocabulary to create a tokenized input sentence which is used as input for the model\n",
    "\n",
    "#### Comparing tokenization strategies\n",
    "##### Word\n",
    "Most common and intuitive, makes sense from our view as we generally break down sentences into words as well. An issue comes when model looks at a unknown word which can be replaced with unknown tokens however these unknown tokens have no distinction. Generally pre-trained models just have a large vocab but this can become an issue.\n",
    "##### Character\n",
    "Fix the drawback of word tokenization and limit vocabulary to just 26. Issue comes from size of input and output as they are now going character by character, this also makes it harder to find relationships between words\n",
    "##### Sub-Word\n",
    "Uses sub-words, best description I can give of this is splitting prefix and suffix and the inbetweens, there are more things but generally thats how it works.\n",
    "\n",
    "It seems as though sub-words is probably the best strategy as the article does not go into its drawbacks.\n",
    "\n",
    "GPT2 and GPT3 uses byte-size byte-pair tokenizer.\n",
    "GPT3.5 and GPT4 uses tiktoken tokenizer. \n",
    "https://github.com/openai/tiktoken\n",
    "Can also be installed using pip install tiktoken\n",
    "Since GPT seems to be the most popular/latest that I know of, I will copy their tokenizer. It still uses byte-pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pip installations\n",
    "pip install tiktoken\n",
    "pip install torch\n",
    "\n",
    "From HuggingFace to use their datasets\n",
    "pip install transformers\n",
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiktoken Learning\n",
    ".encode() method that converts a text string into a list of token integers.\n",
    "- take length of this for number of tokens\n",
    ".decode() method that converts list off token integers to string\n",
    "- can be lossy (lose data) for tokens on on utf-8 boundaries\n",
    ".decode_single_token_bytes() safely converts a single integer token to the bytes it represents.\n",
    "- b in front indicates they are byte strings\n",
    "\n",
    "This can be used also just to count the tokens of your prompt so you can see how many you are using for chatgpt stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "#Tokenizer \n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\" \n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 1609, 5963, 374, 2294, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare environment\n",
    "import torch\n",
    "from datasets import load_dataset_builder, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Allclan/.cache/huggingface/datasets/allenai___parquet/allenai--soda-354e990899ae2f4a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n",
       "    num_rows: 1191582\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset loading\n",
    "\n",
    "dataset = load_dataset(\"allenai/soda\", split=\"train\")\n",
    "\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8f870ccdf45fc3d3b85f6c2f5c8ea5adac087385e77203f97ab6c77681684c8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.10 ('mlvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
